{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 10719932,
          "sourceType": "datasetVersion",
          "datasetId": 6644908,
          "isSourceIdPinned": true
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Speech Translation Fon vers Français avec Wav2Vec2 et MBart**\n",
        "\n",
        "---\n",
        "\n",
        "## **Description du notebook**\n",
        "\n",
        "Ce notebook implémente un modèle de traduction vocale de bout en bout (End-to-End) pour traduire la langue **Fon** vers le **français**. Il utilise l'architecture suivante :\n",
        "\n",
        "*   **Encodeur Audio :** [Wav2Vec2-Base](https://huggingface.co/facebook/wav2vec2-base) pré-entraîné de Facebook (gelé pendant l'entraînement).\n",
        "*   **Adapter Audio :** Un adapter pour adapter les représentations de Wav2Vec2 au décodeur.\n",
        "*   **Décodeur de Traduction :** [MBart-Large-CC25](https://huggingface.co/facebook/mbart-large-cc25) pré-entraîné de Facebook (partiellement gelé, les 6 premières couches sont gelées).\n",
        "*   **Classe Personnalisée** `SpeechTranslationModel` :\n",
        "\n",
        "\n",
        "Le modèle a été fine-tuné sur un dataset de traduction vocale Fon-Fr FFTSC 2025 fourni dans le cadre de l'IWSTL 2025\n",
        "\n",
        "\n",
        "**Ce notebook vous permet de :**\n",
        "\n",
        "*   **Explorer et d'éxecuter le code d'entraînement**\n",
        "*   **Évaluer le modèle** sur un dataset de validation.\n",
        "*   **Tester la traduction vocale** sur vos propres fichiers audio Fon.\n",
        "\n"
      ],
      "metadata": {
        "id": "seKKQMrcLYxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chargement des données"
      ],
      "metadata": {
        "id": "IfjTN3upLYxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chargement de mon Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9HV1TCmz2NBX",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:13:01.020756Z",
          "iopub.execute_input": "2025-03-26T05:13:01.021069Z",
          "iopub.status.idle": "2025-03-26T05:13:01.024674Z",
          "shell.execute_reply.started": "2025-03-26T05:13:01.021045Z",
          "shell.execute_reply": "2025-03-26T05:13:01.023838Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Install requirements\n",
        "%%capture\n",
        "!pip install -q transformers datasets sacremoses sacrebleu"
      ],
      "metadata": {
        "trusted": true,
        "id": "t-pczxRGNahP",
        "execution": {
          "iopub.status.busy": "2025-03-26T05:13:01.061644Z",
          "iopub.execute_input": "2025-03-26T05:13:01.061892Z",
          "iopub.status.idle": "2025-03-26T05:13:06.041349Z",
          "shell.execute_reply.started": "2025-03-26T05:13:01.061871Z",
          "shell.execute_reply": "2025-03-26T05:13:06.040267Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Préparation des données"
      ],
      "metadata": {
        "id": "8Hd5noZoLYxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lignes directives :**\n",
        " - Configuration du path\n",
        " - Chargement des DF\n",
        " - Exploration et analyse des DF"
      ],
      "metadata": {
        "id": "rP-MmdfWLYxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODE_DEBUG = False # Pour desactiver ou activer certaines parties du code"
      ],
      "metadata": {
        "id": "vAqM-W4bEpr3",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:13:06.042697Z",
          "iopub.execute_input": "2025-03-26T05:13:06.043052Z",
          "iopub.status.idle": "2025-03-26T05:13:06.046969Z",
          "shell.execute_reply.started": "2025-03-26T05:13:06.043019Z",
          "shell.execute_reply": "2025-03-26T05:13:06.046060Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa"
      ],
      "metadata": {
        "id": "Pv8fLTEDRkT-",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:13:06.048636Z",
          "iopub.execute_input": "2025-03-26T05:13:06.048854Z",
          "iopub.status.idle": "2025-03-26T05:13:06.381424Z",
          "shell.execute_reply.started": "2025-03-26T05:13:06.048836Z",
          "shell.execute_reply": "2025-03-26T05:13:06.380541Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration du path\n",
        "BASE_PATH = \"/content/drive/MyDrive/dataset\"\n",
        "\n",
        "TRAIN_PATH = os.path.join(BASE_PATH, \"train.csv\")\n",
        "TRAIN_AUDIO_DIR = os.path.join(BASE_PATH, \"train\")\n",
        "\n",
        "VALID_PATH = os.path.join(BASE_PATH, \"valid.csv\")\n",
        "VALID_AUDIO_DIR = os.path.join(BASE_PATH, \"valid\")"
      ],
      "metadata": {
        "id": "RSNVSH4NSVF0",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:13:06.382717Z",
          "iopub.execute_input": "2025-03-26T05:13:06.383166Z",
          "iopub.status.idle": "2025-03-26T05:13:06.387182Z",
          "shell.execute_reply.started": "2025-03-26T05:13:06.383134Z",
          "shell.execute_reply": "2025-03-26T05:13:06.386384Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# CHARGEMENT DES DF\n",
        "train_df = pd.read_csv(TRAIN_PATH)\n",
        "valid_df = pd.read_csv(VALID_PATH)\n",
        "\n",
        "train_df.info()\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "valid_df.info()"
      ],
      "metadata": {
        "id": "jvpvuE5qS7iA",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:13:06.387965Z",
          "iopub.execute_input": "2025-03-26T05:13:06.388169Z",
          "iopub.status.idle": "2025-03-26T05:13:06.641693Z",
          "shell.execute_reply.started": "2025-03-26T05:13:06.388141Z",
          "shell.execute_reply": "2025-03-26T05:13:06.641029Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Sum total de la durée des audios\n",
        "print(\"{:.2f}h d'audios dans train\".format(sum(train_df['duration'])/3600))\n",
        "print(\"{:.2f}h d'audios dans valid\".format(sum(valid_df['duration'])/3600))"
      ],
      "metadata": {
        "id": "sWmqdl7vZlNu",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:13:06.642437Z",
          "iopub.execute_input": "2025-03-26T05:13:06.642642Z",
          "iopub.status.idle": "2025-03-26T05:13:06.649646Z",
          "shell.execute_reply.started": "2025-03-26T05:13:06.642623Z",
          "shell.execute_reply": "2025-03-26T05:13:06.649063Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.sample(5)"
      ],
      "metadata": {
        "id": "gKgjHSDzHcWN",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:13:06.650347Z",
          "iopub.execute_input": "2025-03-26T05:13:06.650624Z",
          "iopub.status.idle": "2025-03-26T05:13:06.684511Z",
          "shell.execute_reply.started": "2025-03-26T05:13:06.650596Z",
          "shell.execute_reply": "2025-03-26T05:13:06.683590Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "valid_df.sample(5)"
      ],
      "metadata": {
        "id": "WpnjXxLjHa0a",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:13:06.686711Z",
          "iopub.execute_input": "2025-03-26T05:13:06.686934Z",
          "iopub.status.idle": "2025-03-26T05:13:06.695091Z",
          "shell.execute_reply.started": "2025-03-26T05:13:06.686915Z",
          "shell.execute_reply": "2025-03-26T05:13:06.694243Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graphiques pour mieux appréhender les données"
      ],
      "metadata": {
        "id": "rYD5jxfpGsLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "train_df[\"utterance_length\"] = train_df[\"utterance\"].apply(lambda x: len(x.split()))\n",
        "valid_df[\"utterance_length\"] = valid_df[\"utterance\"].apply(lambda x: len(x.split()))\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.histplot(train_df[\"utterance_length\"], bins=30, kde=False, color=\"blue\", alpha=0.6, label=\"Train\")\n",
        "sns.histplot(valid_df[\"utterance_length\"], bins=30, kde=False, color=\"orange\", alpha=0.6, label=\"Validation\")\n",
        "plt.xlabel(\"Nombre de mots\")\n",
        "plt.ylabel(\"Nombre d'échantillons\")\n",
        "plt.title(\"Comparaison de la taille des transcriptions (Train vs Validation)\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wIAgeqgaGsyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Histogramme des durées des audios\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(train_df[\"duration\"], bins=50, alpha=0.7, color=\"blue\", label=\"Train\", edgecolor=\"black\")\n",
        "plt.hist(valid_df[\"duration\"], bins=50, alpha=0.7, color=\"orange\", label=\"Validation\", edgecolor=\"black\")\n",
        "\n",
        "# Titre et labels\n",
        "plt.title(\"Comparaison de la durée des audios (Train vs Validation)\")\n",
        "plt.xlabel(\"Durée des audios (secondes)\")\n",
        "plt.ylabel(\"Nombre d'échantillons\")\n",
        "plt.legend()\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "\n",
        "# Affichage\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qGkK3BBqGxdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train - Durée moyenne:\", train_df[\"duration\"].mean(), \"sec\")\n",
        "print(\"Train - Durée médiane:\", train_df[\"duration\"].median(), \"sec\")\n",
        "\n",
        "print(\"Validation - Durée moyenne:\", valid_df[\"duration\"].mean(), \"sec\")\n",
        "print(\"Validation - Durée médiane:\", valid_df[\"duration\"].median(), \"sec\")"
      ],
      "metadata": {
        "id": "sEA2VAN9G0pH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **On peut remarquer que la plupart des audios dans le dataset(train/validation)\n",
        " mis à disposition font moins de 6 secondes. Vu les contraintes en ressources de calcul (GPU T4 15Gb valable pour 4h : insuffisant pour le travail), je vais cibler uniquement les audios courts (<=5s) pour cette occasion.**\n",
        "\n"
      ],
      "metadata": {
        "id": "CTP0y9p6G359"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# On utilise desormais une version de train_df et valid_df qui ne contienent que des audios dont la durée est inférieure ou également à 5s\n",
        "train_df = train_df[train_df[\"duration\"] <= 5]\n",
        "valid_df = valid_df[valid_df[\"duration\"] <= 5]\n",
        "train_df.info()"
      ],
      "metadata": {
        "id": "NdNyD_mLG9hB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretraitez le texte\n",
        "import re\n",
        "\n",
        "def pretraitement_text(text):\n",
        "  text = text.lower()\n",
        "\n",
        "  # Nettoie : espaces multiples, parenthèses et symboles arithmétiques\n",
        "  text = re.sub(r\"\\s+\", \" \", text)\n",
        "  text = re.sub(r'[()=+*/\\\\[\\]]', '', text)\n",
        "  text = text.strip()\n",
        "\n",
        "  return text\n",
        "\n",
        "train_df['utterance'] = train_df['utterance'].apply(pretraitement_text)\n",
        "valid_df['utterance'] = valid_df['utterance'].apply(pretraitement_text)\n",
        "\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "U_w_T6bhT2qG",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:13:06.696839Z",
          "iopub.execute_input": "2025-03-26T05:13:06.697035Z",
          "iopub.status.idle": "2025-03-26T05:13:06.902898Z",
          "shell.execute_reply.started": "2025-03-26T05:13:06.697018Z",
          "shell.execute_reply": "2025-03-26T05:13:06.902205Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# On ajoute le chemin complet vers les fichiers audio\n",
        "def add_audio_path(df, DIR, col_name=\"filename\"):\n",
        "    df[\"audio\"] = df[col_name].apply(lambda filename: os.path.join(DIR, filename))\n",
        "    return df\n",
        "\n",
        "train_df = add_audio_path(train_df, TRAIN_AUDIO_DIR)\n",
        "valid_df = add_audio_path(valid_df, VALID_AUDIO_DIR)\n",
        "\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "amOzZo5pTsTZ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:13:06.903627Z",
          "iopub.execute_input": "2025-03-26T05:13:06.903889Z",
          "iopub.status.idle": "2025-03-26T05:13:06.948285Z",
          "shell.execute_reply.started": "2025-03-26T05:13:06.903869Z",
          "shell.execute_reply": "2025-03-26T05:13:06.947668Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "valid_df.head()"
      ],
      "metadata": {
        "id": "mSTC7VMq4L7N",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:13:06.948995Z",
          "iopub.execute_input": "2025-03-26T05:13:06.949220Z",
          "iopub.status.idle": "2025-03-26T05:13:06.956877Z",
          "shell.execute_reply.started": "2025-03-26T05:13:06.949189Z",
          "shell.execute_reply": "2025-03-26T05:13:06.956253Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"duration\"].mean()"
      ],
      "metadata": {
        "id": "5P2Ncn9drdNx",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:13:06.957516Z",
          "iopub.execute_input": "2025-03-26T05:13:06.957689Z",
          "iopub.status.idle": "2025-03-26T05:13:06.972007Z",
          "shell.execute_reply.started": "2025-03-26T05:13:06.957673Z",
          "shell.execute_reply": "2025-03-26T05:13:06.971310Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "if MODE_DEBUG:\n",
        "  # Vérification de l'existence des fichiers audio\n",
        "  def check_file_exists(row):\n",
        "      \"\"\"Vérifie si le fichier audio existe, en tenant compte du chemin de base.\"\"\"\n",
        "      file_path = row['audio']['path']\n",
        "      return os.path.exists(file_path)\n",
        "\n",
        "  print(\"Vérification de l'existence des fichiers audio...\")\n",
        "\n",
        "  for df in [train_df, valid_df]:\n",
        "    df['files_exists'] = df.apply(check_file_exists, axis=1)\n",
        "    missing_files = df[~df['files_exists']]\n",
        "\n",
        "    if not missing_files.empty:\n",
        "        print(f\"Avertissement : {len(missing_files)} fichiers audio n'ont pas été trouvés.\")\n",
        "    else:\n",
        "        print(\"Tous les fichiers audio sont accessibles.\")"
      ],
      "metadata": {
        "id": "xpDybitUZJAI",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:13:06.972693Z",
          "iopub.execute_input": "2025-03-26T05:13:06.973002Z",
          "iopub.status.idle": "2025-03-26T05:13:06.986316Z",
          "shell.execute_reply.started": "2025-03-26T05:13:06.972983Z",
          "shell.execute_reply": "2025-03-26T05:13:06.985516Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "if MODE_DEBUG:\n",
        "  # Vérification de la cohérence des durées audio\n",
        "  def verify_duration(row, tolerance=0.1):\n",
        "      \"\"\"\n",
        "      Vérifie que la durée réelle du fichier audio correspond à celle indiquée dans le CSV.\n",
        "      \"\"\"\n",
        "      file_path = row['audio']['path']\n",
        "\n",
        "      try:\n",
        "          # Chargement de l'audio sans rééchantillonnage\n",
        "          audio, sr = librosa.load(file_path, sr=None)\n",
        "          real_duration = librosa.get_duration(y=audio, sr=sr)\n",
        "          expected_duration = float(row['duration'])\n",
        "          return abs(real_duration - expected_duration) <= tolerance\n",
        "      except Exception as e:\n",
        "          print(f\"Erreur lors du chargement du fichier {file_path} : {e}\")\n",
        "          return False\n",
        "\n",
        "  print(\"Vérification de la cohérence des durées audio...\")\n",
        "\n",
        "  for df in [train_df, valid_df]:\n",
        "    df['duration_match'] = df.apply(verify_duration, axis=1)\n",
        "\n",
        "    mismatched = df[~df['duration_match']]\n",
        "\n",
        "    if not mismatched.empty:\n",
        "        print(f\"Avertissement : {len(mismatched)} fichiers présentent une durée incohérente.\")\n",
        "    else:\n",
        "        print(\"Les durées audio correspondent aux valeurs attendues.\")"
      ],
      "metadata": {
        "id": "Xf_SScrCbqjG",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:13:06.987191Z",
          "iopub.execute_input": "2025-03-26T05:13:06.987482Z",
          "iopub.status.idle": "2025-03-26T05:13:07.004067Z",
          "shell.execute_reply.started": "2025-03-26T05:13:06.987445Z",
          "shell.execute_reply": "2025-03-26T05:13:07.003338Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Audio lazy loading\n",
        "from datasets import Dataset, Audio\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "valid_dataset = Dataset.from_pandas(valid_df)\n",
        "\n",
        "train = train_dataset.train_test_split(test_size=0.95)[\"train\"] # Pour éviter crash de session dû à la RAM\n",
        "valid = valid_dataset.train_test_split(test_size=0.95)[\"train\"]\n",
        "\n",
        "train_subset = train.cast_column(\"audio\", Audio())\n",
        "valid_subset = valid.cast_column(\"audio\", Audio())"
      ],
      "metadata": {
        "trusted": true,
        "id": "jAZwilybNahV",
        "execution": {
          "iopub.status.busy": "2025-03-26T05:13:07.004822Z",
          "iopub.execute_input": "2025-03-26T05:13:07.005021Z",
          "iopub.status.idle": "2025-03-26T05:13:08.516124Z",
          "shell.execute_reply.started": "2025-03-26T05:13:07.004991Z",
          "shell.execute_reply": "2025-03-26T05:13:08.515480Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_subset), len(valid_subset)"
      ],
      "metadata": {
        "id": "wpc0WBozHE2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vérification manuelle\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "random_row = train_subset[9]\n",
        "audio_array = random_row[\"audio\"][\"array\"]\n",
        "\n",
        "print(\"utterance : \", random_row[\"utterance\"])\n",
        "display(Audio(audio_array, rate=16000))"
      ],
      "metadata": {
        "id": "v-uvdGb76RQQ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:13:08.542207Z",
          "iopub.execute_input": "2025-03-26T05:13:08.542391Z",
          "iopub.status.idle": "2025-03-26T05:13:20.419117Z",
          "shell.execute_reply.started": "2025-03-26T05:13:08.542373Z",
          "shell.execute_reply": "2025-03-26T05:13:20.418241Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> En explorant rapidement les audios, on remarque qu'il y a des cas de traduction en français pas forcément correctes. Ceci pourrait impacter négativement les perfs du modèle vu qu'il n'avait déjà pas accès à énormenment de ressources de bonne qualité.\n",
        "\n"
      ],
      "metadata": {
        "id": "l2VOV_ODHJ4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conception du modèle de SpeechTranslation"
      ],
      "metadata": {
        "id": "vmoqY65CruMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour construire le modèle, on utilise :\n",
        "- Wav2Vec2.0 pour encoder l'audio\n",
        "- Un adapter pour adapter les représentations de Wav2Vec2 au décodeur.\n",
        "- Une couche de projection transforme les caractéristiques audio pour être compatibles avec le décodeur MBert.\n",
        "- MBert en décodeur pour la traduction en français."
      ],
      "metadata": {
        "id": "x8VPd98UNGsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import Wav2Vec2Model, MBartForConditionalGeneration, MBartTokenizer\n",
        "\n",
        "# class Permute(nn.Module):\n",
        "#     \"\"\"Module personnalisé pour permuter les dimensions d'un tensor\"\"\"\n",
        "#     def __init__(self, dims):\n",
        "#         super().__init__()\n",
        "#         self.dims = dims\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return x.permute(*self.dims)\n",
        "\n",
        "class SpeechTranslationModel(nn.Module):\n",
        "    def __init__(self, encoder_name, decoder_name, tokenizer):\n",
        "        super().__init__()\n",
        "        # Encoder gelé\n",
        "        self.audio_encoder = Wav2Vec2Model.from_pretrained(encoder_name)\n",
        "        for param in self.audio_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Adapter\n",
        "        self.audio_adapter = nn.Sequential(\n",
        "            nn.Conv1d(768, 512, kernel_size=1),  # Réduction dimensionnelle (768 -> 512)\n",
        "            nn.GELU(),\n",
        "            # Permute((0, 2, 1)), # Desactiver pour décharger l'empreinte mémoire\n",
        "            # nn.LayerNorm(512),\n",
        "            # Permute((0, 2, 1))\n",
        "        )\n",
        "\n",
        "        # Décodeur partiellement gelé\n",
        "        self.decoder = MBartForConditionalGeneration.from_pretrained(decoder_name)\n",
        "        for layer in self.decoder.model.decoder.layers[:8]:  # Gel des 6 premières couches\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # Projection pour passer de 512 à d_model du décodeur\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(512, self.decoder.config.d_model)\n",
        "        )\n",
        "\n",
        "        # Tokenizer\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def forward(self, audio_input, audio_attention_mask, text_attention_mask, labels=None):\n",
        "        # Encodage audio (Wav2Vec2 gelé)\n",
        "        with torch.no_grad():\n",
        "            audio_features = self.audio_encoder(audio_input, attention_mask=audio_attention_mask).last_hidden_state  # [batch, seq_len, 768]\n",
        "\n",
        "        # Adaptation des features audio\n",
        "        audio_features = audio_features.permute(0, 2, 1)     # [batch, 768, seq_len]\n",
        "        audio_features = self.audio_adapter(audio_features)  # [batch, 512, seq_len]\n",
        "        audio_features = audio_features.permute(0, 2, 1)     # [batch, seq_len, 512]\n",
        "\n",
        "        # Projection pour matcher la dimension de MBart\n",
        "        decoder_inputs_embeds = self.proj(audio_features)    # [batch, seq_len, d_model]\n",
        "\n",
        "        # Ajustement de la dimension séquentielle\n",
        "        target_seq_len = text_attention_mask.size(1)\n",
        "        decoder_inputs_embeds = decoder_inputs_embeds.transpose(1, 2)  # [batch, d_model, seq_len]\n",
        "        decoder_inputs_embeds = F.interpolate(decoder_inputs_embeds, size=target_seq_len, mode='linear', align_corners=False)\n",
        "        decoder_inputs_embeds = decoder_inputs_embeds.transpose(1, 2)  # [batch, target_seq_len, d_model]\n",
        "\n",
        "        # Décodeur MBart\n",
        "        outputs = self.decoder(\n",
        "            inputs_embeds=decoder_inputs_embeds,\n",
        "            attention_mask=text_attention_mask,\n",
        "            labels=labels,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def generate(self, audio_input, audio_attention_mask, target_seq_len = 150):\n",
        "        # Encodage audio\n",
        "        with torch.no_grad():\n",
        "            audio_features = self.audio_encoder(audio_input, attention_mask=audio_attention_mask).last_hidden_state\n",
        "\n",
        "        # Adaptation + projection\n",
        "        audio_features = self.audio_adapter(audio_features.permute(0, 2, 1))\n",
        "        decoder_inputs_embeds = self.proj(audio_features.permute(0, 2, 1))  # [batch, seq_len, d_model]\n",
        "\n",
        "        decoder_inputs_embeds = decoder_inputs_embeds.transpose(1, 2)\n",
        "        decoder_inputs_embeds = F.interpolate(decoder_inputs_embeds, size=target_seq_len, mode='linear', align_corners=False)\n",
        "        decoder_inputs_embeds = decoder_inputs_embeds.transpose(1, 2)\n",
        "\n",
        "        # Création d'un masque de texte rempli de 1 (pas de masquage)\n",
        "        text_attention_mask = torch.ones(decoder_inputs_embeds.size()[:2], dtype=torch.long, device=decoder_inputs_embeds.device)\n",
        "\n",
        "        # Génération avec MBart\n",
        "        generated_ids = self.decoder.generate(\n",
        "            inputs_embeds=decoder_inputs_embeds,\n",
        "            attention_mask=text_attention_mask,\n",
        "            max_length=target_seq_len,\n",
        "            num_beams=5,\n",
        "            early_stopping=True,\n",
        "            forced_bos_token_id=self.tokenizer.lang_code_to_id[\"fr_XX\"]\n",
        "        )\n",
        "\n",
        "        return generated_ids\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "0Rqe-9-6NahW",
        "execution": {
          "iopub.status.busy": "2025-03-26T05:13:23.336371Z",
          "iopub.execute_input": "2025-03-26T05:13:23.336664Z",
          "iopub.status.idle": "2025-03-26T05:13:38.313523Z",
          "shell.execute_reply.started": "2025-03-26T05:13:23.336633Z",
          "shell.execute_reply": "2025-03-26T05:13:38.312854Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset personnalisé\n",
        "class SpeechTranslationDataset():\n",
        "    def __init__(self, dataset, processor, max_length=8000): # 1/2s\n",
        "        self.dataset = dataset\n",
        "        self.max_length = max_length\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "\n",
        "        # Traitement audio\n",
        "        audio = processor(\n",
        "            item[\"audio\"][\"array\"],\n",
        "            sampling_rate=16000,\n",
        "            return_tensors=\"pt\"\n",
        "        ).input_values.squeeze(0)  # Supprime la dimension de batch\n",
        "\n",
        "        # Troncature/padding\n",
        "        if audio.size(0) < self.max_length:\n",
        "            audio = torch.nn.functional.pad(audio, (0, self.max_length - audio.size(0)))\n",
        "        else:\n",
        "            audio = audio[:self.max_length]\n",
        "\n",
        "        return {\n",
        "            \"audio\": audio,\n",
        "            \"text\": item[\"utterance\"]\n",
        "        }\n",
        "\n",
        "    # Collate Function\n",
        "    def collate_fn(self, batch):\n",
        "        # Audio processsing\n",
        "        audio = [item[\"audio\"] for item in batch]\n",
        "        audio = nn.utils.rnn.pad_sequence(audio, batch_first=True)\n",
        "        audio_attention_mask = (audio != 0).float()\n",
        "\n",
        "        # Texte processing\n",
        "        texts = [item[\"text\"] for item in batch]\n",
        "        max_length = getattr(tokenizer.model_max_length, \"max_length\", 512)\n",
        "        tokenized = tokenizer(\n",
        "            texts,\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        text_attention_mask = tokenized[\"attention_mask\"]\n",
        "\n",
        "        return {\n",
        "            \"audio_input\": audio.to(device),\n",
        "            \"audio_attention_mask\": audio_attention_mask.to(device),\n",
        "            \"labels\": tokenized[\"input_ids\"].to(device),\n",
        "            \"text_attention_mask\": text_attention_mask.to(device)\n",
        "        }"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:13:38.319387Z",
          "iopub.execute_input": "2025-03-26T05:13:38.319697Z",
          "iopub.status.idle": "2025-03-26T05:13:38.358628Z",
          "shell.execute_reply.started": "2025-03-26T05:13:38.319665Z",
          "shell.execute_reply": "2025-03-26T05:13:38.357950Z"
        },
        "id": "0twKuBLNSM66"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Prechargement des données\n",
        "from transformers import Wav2Vec2Processor\n",
        "\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import sacrebleu\n",
        "\n",
        "# Entraînement\n",
        "from torch.optim.adamw import AdamW\n",
        "\n",
        "# Boucle d'entraînement\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Configuration\n",
        "encoder_name = \"facebook/wav2vec2-base\"\n",
        "decoder_name = \"facebook/mbart-large-cc25\"\n",
        "\n",
        "processor = Wav2Vec2Processor.from_pretrained(encoder_name)\n",
        "tokenizer = MBartTokenizer.from_pretrained(decoder_name, tgt_lang=\"fr_XX\")\n",
        "\n",
        "train_subset_dataset = SpeechTranslationDataset(train_subset, processor)\n",
        "train_loader = DataLoader(train_subset_dataset,\n",
        "                          batch_size=2,\n",
        "                          collate_fn=train_subset_dataset.collate_fn,\n",
        "                          shuffle=True)\n",
        "\n",
        "\n",
        "model = SpeechTranslationModel(encoder_name, decoder_name, tokenizer)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "fdbkBYU5NahX",
        "execution": {
          "iopub.status.busy": "2025-03-26T05:13:38.361268Z",
          "iopub.execute_input": "2025-03-26T05:13:38.361472Z",
          "iopub.status.idle": "2025-03-26T05:14:28.842629Z",
          "shell.execute_reply.started": "2025-03-26T05:13:38.361453Z",
          "shell.execute_reply": "2025-03-26T05:14:28.841430Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Initalisation de wandb\n",
        "import wandb\n",
        "\n",
        "# Initialisation\n",
        "wandb.login(key=\"\")\n",
        "\n",
        "wandb.init(project=\"st-FonFr\",\n",
        "           config={\"architecture\": \"Wav2Vec2-MBart\",\n",
        "                   \"dataset\": \"FFSTC_IWSLT2025\",\n",
        "                    \"encoder\": \"wav2vec2-base\",\n",
        "                    \"decoder\": \"mbart-large-cc25\",\n",
        "                    \"lr\": 5e-5,\n",
        "                    \"batch_size\": 2\n",
        "                   })\n",
        "\n",
        "wandb.watch(model, log=\"all\", log_freq=50)"
      ],
      "metadata": {
        "id": "28Ro-vNLHe0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrainement proprement dit\n",
        "from datetime import timedelta\n",
        "import time\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "losses_per_epoch = []  # loss moyenne par epoch\n",
        "perplexities = []      # perplexité par epoch\n",
        "time_per_epoch = []    # Temps d'exécution par epoch\n",
        "\n",
        "start_time = time.time()\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "\n",
        "def calculate_perplexity(loss):\n",
        "    return torch.exp(torch.tensor(loss)) # exp(loss)\n",
        "\n",
        "# Boucle d'entrainement\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    step = 0\n",
        "\n",
        "    epoch_start_time = time.time()\n",
        "    print(f\" Epoch : {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        current_step = epoch * len(train_loader) + batch_idx\n",
        "\n",
        "        # Forward + backward\n",
        "        outputs = model(\n",
        "            batch[\"audio_input\"].to(device),\n",
        "            audio_attention_mask=batch[\"audio_attention_mask\"].to(device),\n",
        "            labels=batch[\"labels\"].to(device),\n",
        "            text_attention_mask=batch[\"text_attention_mask\"].to(device)\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        # Mise à jour des poids tous les 5 steps (economie de  GPU)\n",
        "        if (batch_idx + 1) % 5 == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # Calcul des métriques\n",
        "        total_loss += loss.item()\n",
        "        avg_loss = total_loss / (batch_idx + 1)\n",
        "        perplexity = calculate_perplexity(avg_loss).item()\n",
        "\n",
        "        # Calcul du temps écoulé et restant\n",
        "        elapsed = time.time() - start_time\n",
        "        steps_per_sec = elapsed / (current_step + 1)\n",
        "        remaining = steps_per_sec * (total_steps - current_step - 1)\n",
        "\n",
        "        # Affichage tous les 10 steps\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{EPOCHS} | Step {batch_idx+1}/{len(train_loader)} | \"\n",
        "                  f\"Loss: {loss.item():.4f} | Avg Loss: {avg_loss:.4f} | \"\n",
        "                  f\"Perplexity: {perplexity:.2f} | \"\n",
        "                  f\"Elapsed: {str(timedelta(seconds=int(elapsed)))} | \"\n",
        "                  f\"Remaining: ~{str(timedelta(seconds=int(remaining)))}\",\n",
        "                  end='\\r')\n",
        "\n",
        "        wandb.log({\"train/loss\": loss.item(),\n",
        "                   \"train/avg_loss\": avg_loss,\n",
        "                   \"train/perplexity\": perplexity,\n",
        "                   \"lr\": optimizer.param_groups[0]['lr']},\n",
        "                   step=step + epoch * len(train_loader))\n",
        "\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "    time_per_epoch.append(epoch_time)\n",
        "    losses_per_epoch.append(avg_loss)\n",
        "    perplexities.append(perplexity)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} completed - Avg Loss: {avg_loss:.4f} | Perplexity: {perplexity:.2f} | Time: {epoch_time:.2f} sec\")\n",
        "\n",
        "print(f\"\\nTraining completed in {str(timedelta(seconds=int(time.time() - start_time)))}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "um91ayf6NahX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, EPOCHS + 1), losses_per_epoch, marker='o', linestyle='-', color='b')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Average Loss\")\n",
        "plt.title(\"Loss Evolution\")\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, EPOCHS + 1), perplexities, marker='s', linestyle='-', color='r')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Perplexity\")\n",
        "plt.title(\"Perplexity Evolution\")\n",
        "plt.grid()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0RPuDiJ3Hmu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> On remarque que la courbe de perte du modèle dimunie progressivement au fil des epochs, on peut donc dire que"
      ],
      "metadata": {
        "id": "mOtxStGFHo_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sacrebleu import corpus_bleu\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def evaluate(model, dataset):\n",
        "    model.eval()\n",
        "    references = []  # Format: [ [ref1], [ref2], ... ]\n",
        "    hypotheses = []\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=2, collate_fn=dataset.collate_fn)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluation\"):\n",
        "            # Génération\n",
        "            generated_ids = model.generate(\n",
        "                batch[\"audio_input\"],\n",
        "                batch[\"audio_attention_mask\"]\n",
        "            )\n",
        "\n",
        "            # Décodage des prédictions\n",
        "            preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "            hypotheses.extend(preds)\n",
        "\n",
        "            # Décodage des références (à partir des input_ids)\n",
        "            refs = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
        "            references.extend([[r] for r in refs])  # Format SacreBLEU\n",
        "\n",
        "    bleu = corpus_bleu(hypotheses, references)\n",
        "    return {\"bleu\": bleu.score, \"predictions\": hypotheses, \"references\": references}\n",
        "\n",
        "# Utilisation\n",
        "valid_dataset = SpeechTranslationDataset(valid_subset, processor)\n",
        "results = evaluate(model, valid_dataset)\n",
        "print(f\"BLEU Score: {results['bleu']}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Q-XMEPRnNahX",
        "execution": {
          "iopub.status.busy": "2025-03-26T05:46:12.756649Z",
          "iopub.execute_input": "2025-03-26T05:46:12.756990Z",
          "iopub.status.idle": "2025-03-26T05:46:53.760601Z",
          "shell.execute_reply.started": "2025-03-26T05:46:12.756966Z",
          "shell.execute_reply": "2025-03-26T05:46:53.759823Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import Wav2Vec2Processor\n",
        "\n",
        "def prepare_audio_input(audio_path, processor, max_length=8000, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    \"\"\"\n",
        "    Transforme un fichier audio en entrée compatible avec votre modèle.\n",
        "\n",
        "    Args:\n",
        "        audio_path (str): Chemin vers le fichier audio (.wav)\n",
        "        processor (Wav2Vec2Processor): Processor pour la normalisation audio\n",
        "        max_length (int): Longueur maximale en échantillons (16kHz = 1 seconde)\n",
        "        device (str): Device pour les tensors (cpu/cuda)\n",
        "\n",
        "    Returns:\n",
        "        dict: {\"audio_input\": tensor, \"audio_attention_mask\": tensor}\n",
        "    \"\"\"\n",
        "    # Chargement et resampling si nécessaire\n",
        "    waveform, sample_rate = torchaudio.load(audio_path)\n",
        "    if sample_rate != 16000:\n",
        "        waveform = torchaudio.functional.resample(waveform, sample_rate, 16000)\n",
        "\n",
        "    # Normalisation avec le processor (identique au dataset)\n",
        "    audio = processor(\n",
        "        waveform.squeeze().numpy(),  # Convertir en numpy array\n",
        "        sampling_rate=16000,\n",
        "        return_tensors=\"pt\"\n",
        "    ).input_values.squeeze(0)\n",
        "\n",
        "    # Troncature/Padding\n",
        "    if audio.size(0) < max_length:\n",
        "        audio = torch.nn.functional.pad(audio, (0, max_length - audio.size(0)))\n",
        "    else:\n",
        "        audio = audio[:max_length]\n",
        "\n",
        "    # Création du masque d'attention\n",
        "    audio_attention_mask = (audio != 0).float()\n",
        "\n",
        "    return {\n",
        "        \"audio_input\": audio.unsqueeze(0).to(device),  # Ajoute une dimension batch [1, seq_len]\n",
        "        \"audio_attention_mask\": audio_attention_mask.unsqueeze(0).to(device)\n",
        "    }\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:55:26.568721Z",
          "iopub.execute_input": "2025-03-26T05:55:26.569048Z",
          "iopub.status.idle": "2025-03-26T05:55:27.277093Z",
          "shell.execute_reply.started": "2025-03-26T05:55:26.569022Z",
          "shell.execute_reply": "2025-03-26T05:55:27.276117Z"
        },
        "id": "KOQKTrf8SM6_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2Processor\n",
        "\n",
        "# processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "\n",
        "audio_input = prepare_audio_input(\"/kaggle/input/ffstc-2025/train/19404.wav\", processor)\n",
        "\n",
        "# Génération avec le modèle\n",
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        audio_input=audio_input[\"audio_input\"],\n",
        "        audio_attention_mask=audio_input[\"audio_attention_mask\"],\n",
        "    )\n",
        "\n",
        "translation = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(translation)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-26T05:56:17.730843Z",
          "iopub.execute_input": "2025-03-26T05:56:17.731133Z",
          "iopub.status.idle": "2025-03-26T05:56:18.404323Z",
          "shell.execute_reply.started": "2025-03-26T05:56:17.731111Z",
          "shell.execute_reply": "2025-03-26T05:56:18.403599Z"
        },
        "id": "wOdum4lzSM7A"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Model"
      ],
      "metadata": {
        "id": "NHp-ObKKHzwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "def save_full_model(model, processor, tokenizer, save_dir=\"st-fonfr\"):\n",
        "    \"\"\"Sauvegarde complète pour production\"\"\"\n",
        "    save_path = Path(save_dir)\n",
        "    save_path.mkdir(exist_ok=True)\n",
        "\n",
        "    # State dict\n",
        "    torch.save(model.state_dict(), save_path / \"model_weights.pth\")\n",
        "\n",
        "    # Processor & Tokenizer\n",
        "    processor.save_pretrained(save_path)\n",
        "    tokenizer.save_pretrained(save_path)\n",
        "\n",
        "    # 3. Metadata minimale\n",
        "    metadata = {\n",
        "        \"model_class\": model.__class__.__name__,\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"audio_sample_rate\": processor.feature_extractor.sampling_rate,\n",
        "        \"target_language\": tokenizer.tgt_lang\n",
        "    }\n",
        "\n",
        "    with open(save_path / \"metadata.json\", \"w\") as f:\n",
        "        json.dump(metadata, f)\n",
        "\n",
        "def load_full_model(save_dir=\"st-fonfr\", device=\"cuda\"):\n",
        "    save_path = Path(save_dir)\n",
        "\n",
        "    # Chargement processor/tokenizer\n",
        "    processor = Wav2Vec2Processor.from_pretrained(save_path)\n",
        "    tokenizer = MBartTokenizer.from_pretrained(save_path)\n",
        "\n",
        "    # Reconstruction du modèle vide\n",
        "    model = SpeechTranslationModel(processor, tokenizer).to(device)\n",
        "\n",
        "    # Chargement des poids\n",
        "    model.load_state_dict(torch.load(save_path / \"model_weights.pth\", map_location=device))\n",
        "\n",
        "    return model, processor, tokenizer"
      ],
      "metadata": {
        "id": "GIOOJiazH1ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAUVEGARDE DU MODELE EN LOCAL\n",
        "save_full_model(model, processor, tokenizer)"
      ],
      "metadata": {
        "id": "Ke5yug4nH4Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload vers HuggingFace"
      ],
      "metadata": {
        "id": "W-uJRiB7H_i9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip -s install huggingface_hub"
      ],
      "metadata": {
        "id": "ytl0Vm46H9kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "qNhRIJ-8ICV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_folder\n",
        "\n",
        "local_model_path = \"st-FonFr\"\n",
        "repo_id = \"TitanSage02/st-wav2vec2-mbart-fon-fr\"\n",
        "\n",
        "upload_folder(\n",
        "    repo_id=repo_id,\n",
        "    folder_path=local_model_path,\n",
        "    commit_message=\"Upload du modèle Speech Translation Fon-Fr\"\n",
        ")\n",
        "\n",
        "print(f\"Modèle uploadé avec succès vers le Hugging Face Hub : https://huggingface.co/{repo_id}\")"
      ],
      "metadata": {
        "id": "3WMaPysnIE4S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}