{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2W3LC3prKIU"
      },
      "source": [
        "# Modèle de Speech-to-Text pour le Fon\n",
        "\n",
        "Ce notebook propose une base de départ pour construire un système de reconnaissance vocale pour le Fon.\n",
        "\n",
        "**Objectifs :**\n",
        " - Préparer un dataset de Fon (audio et transcription).\n",
        " - Extraire des caractéristiques audio, en intégrant éventuellement des informations sur le pitch pour capturer l’intonation.\n",
        " - Utiliser un modèle pré-entraîné (whisper).\n",
        " - Fine-tuner le modèle sur le dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3-Cx4uKweCR"
      },
      "outputs": [],
      "source": [
        "# Chargement de mon Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lue5wWjdrKIi"
      },
      "source": [
        "# Importation des packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdLzsnFVrKIm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lZEmu-frKIr"
      },
      "outputs": [],
      "source": [
        "DATASET_DIR = Path(\"/content/drive/MyDrive/dataset\")\n",
        "TRAIN_AUDIO_DIR = DATASET_DIR / \"train\"\n",
        "VALIDATION_AUDIO_DIR = DATASET_DIR / \"valid\"\n",
        "\n",
        "TRAIN_CSV_PATH = DATASET_DIR / \"train.csv\"\n",
        "VALIDATION_CSV_PATH = DATASET_DIR / \"valid.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nr_V3ACrKIu"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
        "valid_df = pd.read_csv(VALIDATION_CSV_PATH)\n",
        "\n",
        "train_df.info()\n",
        "train_df.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjo4yEorrKIx"
      },
      "source": [
        "# Chargement et préparation du dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfDKlCSTvj1J"
      },
      "outputs": [],
      "source": [
        "!pip -q install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXGd2knRrKI0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datasets import Dataset, Audio\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "valid_dataset = Dataset.from_pandas(valid_df)\n",
        "\n",
        "train_dataset = train_dataset.train_test_split(test_size=0.95)[\"train\"] # Pour éviter crash de session dû à la RAM\n",
        "valid_dataset = valid_dataset.train_test_split(test_size=0.95)[\"train\"]\n",
        "\n",
        "\n",
        "# On ajoute le chemin complet vers les fichiers audio\n",
        "def add_audio_path(DIR, row):\n",
        "    row[\"audio\"] = os.path.join(str(DIR), row[\"filename\"])\n",
        "    return row\n",
        "\n",
        "train_dataset = train_dataset.map(lambda row: add_audio_path(TRAIN_AUDIO_DIR, row))\n",
        "valid_dataset = valid_dataset.map(lambda row: add_audio_path(VALIDATION_AUDIO_DIR, row))\n",
        "\n",
        "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "valid_dataset = valid_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K5Ecb4yrKI3"
      },
      "source": [
        "# Préparation du modèle et du processor (Whisper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shWJKz3CrKI7"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "\n",
        "model_name = \"openai/whisper-tiny\"\n",
        "processor = WhisperProcessor.from_pretrained(model_name)\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kalCg-X1rKJB"
      },
      "source": [
        "# Prétraitement des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a5h_u1urKJE"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(batch):\n",
        "    audio_arrays = [x[\"array\"] for x in batch[\"audio\"]]\n",
        "\n",
        "    inputs = processor.feature_extractor(audio_arrays, sampling_rate=16000, return_tensors=\"np\") # Extraction des features audio\n",
        "\n",
        "    texts = [\"translate: \" + txt for txt in batch[\"utterance\"]] # On ajoute \"translate : \" devant chaque texte\n",
        "    labels = processor.tokenizer(texts, return_tensors=\"np\").input_ids\n",
        "\n",
        "    batch[\"input_features\"] = inputs.input_features\n",
        "    batch[\"labels\"] = labels\n",
        "    return batch\n",
        "\n",
        "# Prétraitement des données\n",
        "dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZdtSePBrKJH"
      },
      "source": [
        "# Datacollector personnalisé"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6rITD1-rKJI"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2Seq:\n",
        "    processor: WhisperProcessor\n",
        "\n",
        "    def __call__(self, features):\n",
        "        # Regroupement et padding des input_features\n",
        "        input_features = [f[\"input_features\"] for f in features]\n",
        "        batch_input = self.processor.feature_extractor.pad(\n",
        "            {\"input_features\": input_features}, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Regroupement et padding des labels\n",
        "        labels = [f[\"labels\"] for f in features]\n",
        "        batch_labels = self.processor.tokenizer.pad(\n",
        "            {\"input_ids\": labels}, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        batch_labels[\"input_ids\"][batch_labels[\"input_ids\"] == self.processor.tokenizer.pad_token_id] = -100 # padding token par -100 pour les éviter dans la loss\n",
        "\n",
        "        batch = {\n",
        "            \"input_features\": batch_input[\"input_features\"],\n",
        "            \"labels\": batch_labels[\"input_ids\"]\n",
        "        }\n",
        "\n",
        "        return batch\n",
        "\n",
        "data_collator = DataCollatorSpeechSeq2Seq(processor=processor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWKo72efrKJK"
      },
      "source": [
        "# Métriques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAKYhXCNrKJM"
      },
      "outputs": [],
      "source": [
        "!pip -q install evaluate sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BO3WTh3HrKJM"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "bleu_metric = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    \"\"\"\n",
        "    Cette fonction reçoit les prédictions et les labels sous forme de tuples,\n",
        "    décode les séquences et calcule le score BLEU.\n",
        "    \"\"\"\n",
        "    pred_ids, label_ids = eval_preds\n",
        "\n",
        "    # Décodage des prédictions en textes\n",
        "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Remplace les tokens -100 par le token de padding pour le décodage des labels\n",
        "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Le format attendu pour sacreBLEU : une liste de références par prédiction (ici, une seule référence par prédiction)\n",
        "    references = [[ref] for ref in label_str]\n",
        "\n",
        "    # Calcul du score BLEU\n",
        "    result = bleu_metric.compute(predictions=pred_str, references=references)\n",
        "    return {\"bleu\": result[\"score\"]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvmS7sMprKJP"
      },
      "source": [
        "# Configuration de l'env d'entrainement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0DTNAr1rKJQ"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-fon-fr\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    learning_rate=1e-4,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_total_limit=3,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    logging_steps=NUM_EPOCHS%5,\n",
        "    push_to_hub=False,\n",
        "    run_name=\"Fongbe_Experiment\"\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz8_nETyrKJR"
      },
      "source": [
        "# Config de wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMGpiN9YrKJS"
      },
      "outputs": [],
      "source": [
        "!pip install -q wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mT9HsSpyrKJU"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login(key=\"\")\n",
        "\n",
        "wandb.init(project=\"IWSLT_Fongbe\", name=\"exp1_baseline\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLTQMwQWrKJX"
      },
      "source": [
        "# Entrainement du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2RbT3htrKJZ"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWItPZ5CrKJZ"
      },
      "source": [
        "# Test sur un échantillon de l'ensemble de validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewylckWmrKJb"
      },
      "outputs": [],
      "source": [
        "results = trainer.evaluate()\n",
        "print(\"Résultats sur l'ensemble de validation :\", results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGtJ_xqqrKJc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "sample = dataset[\"validation\"][0]\n",
        "\n",
        "correct_translate = sample[\"utterance\"]\n",
        "\n",
        "# On prépare les input_features au format tensor\n",
        "input_features = torch.tensor(sample[\"input_features\"]).unsqueeze(0)\n",
        "\n",
        "# Génération de la traduction à partir des input_features\n",
        "generated_ids = model.generate(input_features, max_length=100)\n",
        "translation = processor.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "print(\"Traduction attendue : \", correct_translate)\n",
        "print(\"Traduction générée : \", translation)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 6644908,
          "sourceId": 10719932,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30887,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
